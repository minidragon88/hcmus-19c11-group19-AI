{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdc9D156gB1A",
        "colab_type": "text"
      },
      "source": [
        "Prepare dependencies and global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGI_LAHQC4SS",
        "colab_type": "code",
        "outputId": "60ad3a1e-2d8e-4fe2-9394-e8ee62c57df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "import re\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from collections import defaultdict\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "os.environ['KERAS_BACKEND']='theano'\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "TSV_DIR = \"/content/drive/My Drive/Text Classification/Testground/data/imdb/labeledTrainData.tsv\"\n",
        "GLOVE_DIR = \"/content/drive/My Drive/Text Classification/Testground/data/glove/\"\n",
        "TRAINED_MODEL_DIR = \"/content/drive/My Drive/Text Classification/Testground/model/\""
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s3I9mLfgPhC",
        "colab_type": "text"
      },
      "source": [
        "Prepare dependent functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgofIXFEbQCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_data(dir):\n",
        "  data_train = pd.read_csv(dir, sep='\\t')\n",
        "  return data_train\n",
        "\n",
        "def get_pretrained_glove_vector(dir):\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(dir, 'glove.6B.100d.txt'))\n",
        "\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))\n",
        "  return embeddings_index\n",
        "\n",
        "# remove some unwanted characters\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for dataset\n",
        "    Every dataset is lower cased except\n",
        "    \"\"\"\n",
        "    string = re.sub(b'\\\\\\\\', b'', string)   \n",
        "    string = re.sub(b\"\\'\", b\"\", string)    \n",
        "    string = re.sub(b\"\\\"\", b\"\", string)    \n",
        "    return string.strip().lower()\n",
        "\n",
        "def standardized_text(data_train):\n",
        "  # parser train data using BeautifulSub and remove html tags\n",
        "  text = BeautifulSoup(data_train.review[idx])\n",
        "  unclean_str = text.get_text().encode('ascii','ignore')\n",
        "  cleaned_str = clean_str(unclean_str)\n",
        "  # decode cleaned string to bytes-like type\n",
        "  return cleaned_str.decode('utf-8')\n",
        "\n",
        "def create_simplified_cnn_network(filter, activate_function=\"relu\"):\n",
        "  l_cov1= Conv1D(128, 5, activation=activate_function)(filter)\n",
        "  l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "  l_cov2 = Conv1D(128, 5, activation=activate_function)(l_pool1)\n",
        "  l_pool2 = MaxPooling1D(5)(l_cov2)\n",
        "  l_cov3 = Conv1D(128, 5, activation=activate_function)(l_pool2)\n",
        "  l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
        "  l_flat = Flatten()(l_pool3)\n",
        "  l_dense = Dense(128, activation=activate_function)(l_flat)\n",
        "  preds = Dense(2, activation='softmax')(l_dense)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['acc'])\n",
        "  return model\n",
        "\n",
        "def create_deeper_cnn_network(filter, activate_function=\"relu\"):\n",
        "  l_concatenate = concatenate(filter, axis=1)\n",
        "  l_cov1= Conv1D(128, 5, activation=activate_function)(l_concatenate)\n",
        "  l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "  l_cov2 = Conv1D(128, 5, activation=activate_function)(l_pool1)\n",
        "  l_pool2 = MaxPooling1D(30)(l_cov2)\n",
        "  l_flat = Flatten()(l_pool2)\n",
        "  l_dense = Dense(128, activation=activate_function)(l_flat)\n",
        "  preds = Dense(2, activation='softmax')(l_dense)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['acc'])\n",
        "  return model\n",
        "\n",
        "def save_model(model, save_dir, name):\n",
        "  if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "  model.save(save_dir + \"/\" + name)\n",
        "\n",
        "def load_model(load_dir, model_name):\n",
        "  return keras.models.load_model(load_dir + \"/\" + model_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg7S5RTRgWjU",
        "colab_type": "text"
      },
      "source": [
        "Prepare data to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYnxG_OZDGPL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2cf09986-2fc5-47be-88b1-3fc8636b9280"
      },
      "source": [
        "data_train = get_train_data(TSV_DIR)\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for idx in range(data_train.review.shape[0]):\n",
        "  texts.append(standardized_text(data_train))\n",
        "  labels.append(data_train.sentiment[idx])\n",
        "    \n",
        "\n",
        "# vectorize the text samples into a 2D integer tensor\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# split the data into a training set and a validation set\n",
        "labels = to_categorical(np.asarray(labels))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "# The training data & label for each data set\n",
        "# training_data  = x_train\n",
        "# training_label = y_train\n",
        "training_data = data[:-nb_validation_samples]\n",
        "training_label = labels[:-nb_validation_samples]\n",
        "\n",
        "# The validation data & label\n",
        "# validation_data     = x_val\n",
        "# validation_label    = y_val\n",
        "validation_data = data[-nb_validation_samples:]\n",
        "validation_label = labels[-nb_validation_samples:]\n",
        "\n",
        "\n",
        "# prepare embedding matrix\n",
        "embeddings_index = get_pretrained_glove_vector(GLOVE_DIR)\n",
        "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector        \n",
        "\n",
        "# load pre-trained word embeddings into an Embedding layer\n",
        "# By setting trainable = True, the network would learn the embedding by itself.     \n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)\n",
        "\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "print(\"...Data is ready to train\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 80566 unique tokens.\n",
            "Shape of data tensor: (25000, 1000)\n",
            "Shape of label tensor: (25000, 2)\n",
            "Total 400000 word vectors in Glove 6B 100d.\n",
            "...Data is ready to train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrhP3dy3eMga",
        "colab_type": "text"
      },
      "source": [
        "Train data using a simplified convolutional approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU1kc6KUeEOX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "b905c932-ca16-450d-a8e3-84910b8cbdc4"
      },
      "source": [
        "# applying a simplified convolutional approach\n",
        "activate_function = \"relu\"\n",
        "model = create_simplified_cnn_network(embedded_sequences, activate_function)\n",
        "print(\"Model fitting - simplified convolutional neural network\")\n",
        "model.summary()\n",
        "model.fit(training_data, training_label, validation_data=(validation_data, validation_label),\n",
        "          epochs=10, batch_size=128)\n",
        "\n",
        "save_model(model, TRAINED_MODEL_DIR, \"simplified_model.h5\")\n",
        "print(\"...Trained data model is saved\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model fitting - simplified convolutional neural network\n",
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 1000, 100)         8056700   \n",
            "_________________________________________________________________\n",
            "conv1d_47 (Conv1D)           (None, 996, 128)          64128     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_47 (MaxPooling (None, 199, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_48 (Conv1D)           (None, 195, 128)          82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_48 (MaxPooling (None, 39, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_49 (Conv1D)           (None, 35, 128)           82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_49 (MaxPooling (None, 1, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 8,301,694\n",
            "Trainable params: 8,301,694\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 656s 33ms/step - loss: 0.6746 - acc: 0.6010 - val_loss: 0.5195 - val_acc: 0.7770\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 654s 33ms/step - loss: 0.4486 - acc: 0.7961 - val_loss: 0.4015 - val_acc: 0.8222\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 654s 33ms/step - loss: 0.3333 - acc: 0.8595 - val_loss: 0.3745 - val_acc: 0.8452\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 654s 33ms/step - loss: 0.2701 - acc: 0.8897 - val_loss: 0.3365 - val_acc: 0.8598\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 653s 33ms/step - loss: 0.2210 - acc: 0.9130 - val_loss: 0.2947 - val_acc: 0.8882\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 655s 33ms/step - loss: 0.1742 - acc: 0.9316 - val_loss: 0.4397 - val_acc: 0.8310\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 653s 33ms/step - loss: 0.1280 - acc: 0.9513 - val_loss: 0.2930 - val_acc: 0.8874\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 654s 33ms/step - loss: 0.0895 - acc: 0.9681 - val_loss: 0.4905 - val_acc: 0.8752\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 654s 33ms/step - loss: 0.0735 - acc: 0.9760 - val_loss: 0.4719 - val_acc: 0.8850\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 652s 33ms/step - loss: 0.0631 - acc: 0.9813 - val_loss: 0.4839 - val_acc: 0.8866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-1Hqy8jeT2s",
        "colab_type": "text"
      },
      "source": [
        "Train data using Deeper Convolutional neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elQGgPUdrqpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3bfc902d-5ea1-49ba-d172-69c123658264"
      },
      "source": [
        "# applying a more complex convolutional approach\n",
        "convs = []\n",
        "filter_sizes = [3,4,5]\n",
        "\n",
        "for fsz in filter_sizes:\n",
        "    l_conv = Conv1D(filters=128,kernel_size=fsz,activation='relu')(embedded_sequences)\n",
        "    l_pool = MaxPooling1D(5)(l_conv)\n",
        "    convs.append(l_pool)\n",
        "\n",
        "activate_function = \"relu\"\n",
        "model = create_deeper_cnn_network(convs, activate_function)\n",
        "print(\"Model fitting - more complex convolutional neural network\")\n",
        "model.summary()\n",
        "model.fit(training_data, training_label, validation_data=(validation_data, validation_label),\n",
        "          epochs=20, batch_size=50)\n",
        "\n",
        "save_model(model, TRAINED_MODEL_DIR, \"deeper_model.h5\")\n",
        "print(\"...Trained data model is saved\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model fitting - more complex convolutional neural network\n",
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 1000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 1000, 100)    8056700     input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_50 (Conv1D)              (None, 998, 128)     38528       embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_51 (Conv1D)              (None, 997, 128)     51328       embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_52 (Conv1D)              (None, 996, 128)     64128       embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_50 (MaxPooling1D) (None, 199, 128)     0           conv1d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_51 (MaxPooling1D) (None, 199, 128)     0           conv1d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_52 (MaxPooling1D) (None, 199, 128)     0           conv1d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 597, 128)     0           max_pooling1d_50[0][0]           \n",
            "                                                                 max_pooling1d_51[0][0]           \n",
            "                                                                 max_pooling1d_52[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_53 (Conv1D)              (None, 593, 128)     82048       concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_53 (MaxPooling1D) (None, 118, 128)     0           conv1d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_54 (Conv1D)              (None, 114, 128)     82048       max_pooling1d_53[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_54 (MaxPooling1D) (None, 3, 128)       0           conv1d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_11 (Flatten)            (None, 384)          0           max_pooling1d_54[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, 128)          49280       flatten_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 2)            258         dense_21[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 8,424,318\n",
            "Trainable params: 8,424,318\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/20\n",
            "20000/20000 [==============================] - 1723s 86ms/step - loss: 0.3631 - acc: 0.8249 - val_loss: 0.2893 - val_acc: 0.8856\n",
            "Epoch 2/20\n",
            "20000/20000 [==============================] - 1718s 86ms/step - loss: 0.1813 - acc: 0.9308 - val_loss: 0.2901 - val_acc: 0.8976\n",
            "Epoch 3/20\n",
            "20000/20000 [==============================] - 1720s 86ms/step - loss: 0.1298 - acc: 0.9522 - val_loss: 0.3102 - val_acc: 0.8914\n",
            "Epoch 4/20\n",
            "20000/20000 [==============================] - 1718s 86ms/step - loss: 0.0966 - acc: 0.9673 - val_loss: 0.3609 - val_acc: 0.8888\n",
            "Epoch 5/20\n",
            "20000/20000 [==============================] - 1716s 86ms/step - loss: 0.0693 - acc: 0.9805 - val_loss: 0.3821 - val_acc: 0.8924\n",
            "Epoch 6/20\n",
            "20000/20000 [==============================] - 1714s 86ms/step - loss: 0.0379 - acc: 0.9891 - val_loss: 0.8213 - val_acc: 0.8820\n",
            "Epoch 7/20\n",
            "20000/20000 [==============================] - 1717s 86ms/step - loss: 0.0253 - acc: 0.9922 - val_loss: 0.5722 - val_acc: 0.8920\n",
            "Epoch 8/20\n",
            "20000/20000 [==============================] - 1715s 86ms/step - loss: 0.0193 - acc: 0.9938 - val_loss: 0.6969 - val_acc: 0.8890\n",
            "Epoch 9/20\n",
            "20000/20000 [==============================] - 1716s 86ms/step - loss: 0.0133 - acc: 0.9962 - val_loss: 0.7189 - val_acc: 0.8842\n",
            "Epoch 10/20\n",
            "20000/20000 [==============================] - 1720s 86ms/step - loss: 0.0133 - acc: 0.9969 - val_loss: 0.8076 - val_acc: 0.8752\n",
            "Epoch 11/20\n",
            " 8000/20000 [===========>..................] - ETA: 16:06 - loss: 0.0113 - acc: 0.9976"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}