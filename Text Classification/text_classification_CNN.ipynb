{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdc9D156gB1A",
        "colab_type": "text"
      },
      "source": [
        "Prepare dependencies and global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGI_LAHQC4SS",
        "colab_type": "code",
        "outputId": "8e20916a-dbc1-4f92-b4d2-5438fae47b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "os.environ['KERAS_BACKEND']='theano'\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "TSV_DIR = \"/content/drive/My Drive/Text Classification/Testground/data/imdb/labeledTrainData.tsv\"\n",
        "GLOVE_DIR = \"/content/drive/My Drive/Text Classification/Testground/data/glove/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s3I9mLfgPhC",
        "colab_type": "text"
      },
      "source": [
        "Prepare dependent functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgofIXFEbQCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_data(dir):\n",
        "  data_train = pd.read_csv(dir, sep='\\t')\n",
        "  return data_train\n",
        "\n",
        "def get_pretrained_glove_vector(dir):\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(dir, 'glove.6B.100d.txt'))\n",
        "\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))\n",
        "  return embeddings_index\n",
        "\n",
        "# remove some unwanted characters\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for dataset\n",
        "    Every dataset is lower cased except\n",
        "    \"\"\"\n",
        "    string = re.sub(b'\\\\\\\\', b'', string)   \n",
        "    string = re.sub(b\"\\'\", b\"\", string)    \n",
        "    string = re.sub(b\"\\\"\", b\"\", string)    \n",
        "    return string.strip().lower()\n",
        "\n",
        "def standardized_text(data_train):\n",
        "  # parser train data using BeautifulSub and remove html tags\n",
        "  text = BeautifulSoup(data_train.review[idx])\n",
        "  unclean_str = text.get_text().encode('ascii','ignore')\n",
        "  cleaned_str = clean_str(unclean_str)\n",
        "  # decode cleaned string to bytes-like type\n",
        "  return cleaned_str.decode('utf-8')\n",
        "\n",
        "def create_simplified_cnn_network(filter, activate_function=\"relu\"):\n",
        "  l_cov1= Conv1D(128, 5, activation=activate_function)(filter)\n",
        "  l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "  l_cov2 = Conv1D(128, 5, activation=activate_function)(l_pool1)\n",
        "  l_pool2 = MaxPooling1D(5)(l_cov2)\n",
        "  l_cov3 = Conv1D(128, 5, activation=activate_function)(l_pool2)\n",
        "  l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
        "  l_flat = Flatten()(l_pool3)\n",
        "  l_dense = Dense(128, activation=activate_function)(l_flat)\n",
        "  preds = Dense(2, activation='softmax')(l_dense)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['acc'])\n",
        "  return model\n",
        "\n",
        "def create_deeper_cnn_network(filter, activate_function=\"relu\"):\n",
        "  l_concatenate = concatenate(filter, axis=1)\n",
        "  l_cov1= Conv1D(128, 5, activation=activate_function)(l_concatenate)\n",
        "  l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "  l_cov2 = Conv1D(128, 5, activation=activate_function)(l_pool1)\n",
        "  l_pool2 = MaxPooling1D(30)(l_cov2)\n",
        "  l_flat = Flatten()(l_pool2)\n",
        "  l_dense = Dense(128, activation=activate_function)(l_flat)\n",
        "  preds = Dense(2, activation='softmax')(l_dense)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['acc'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg7S5RTRgWjU",
        "colab_type": "text"
      },
      "source": [
        "Prepare data to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYnxG_OZDGPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = get_train_data(TSV_DIR)\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for idx in range(data_train.review.shape[0]):\n",
        "  texts.append(standardized_text(data_train))\n",
        "  labels.append(data_train.sentiment[idx])\n",
        "    \n",
        "\n",
        "# vectorize the text samples into a 2D integer tensor\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# split the data into a training set and a validation set\n",
        "labels = to_categorical(np.asarray(labels))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "# The training data & label for each data set\n",
        "# training_data  = x_train\n",
        "# training_label = y_train\n",
        "training_data = data[:-nb_validation_samples]\n",
        "training_label = labels[:-nb_validation_samples]\n",
        "\n",
        "# The validation data & label\n",
        "# validation_data     = x_val\n",
        "# validation_label    = y_val\n",
        "validation_data = data[-nb_validation_samples:]\n",
        "validation_label = labels[-nb_validation_samples:]\n",
        "\n",
        "\n",
        "# prepare embedding matrix\n",
        "embeddings_index = get_pretrained_glove_vector(GLOVE_DIR)\n",
        "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector        \n",
        "\n",
        "# load pre-trained word embeddings into an Embedding layer\n",
        "# By setting trainable = True, the network would learn the embedding by itself.     \n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)\n",
        "\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "print(\"...Data is ready to train\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrhP3dy3eMga",
        "colab_type": "text"
      },
      "source": [
        "Train data using a simplified convolutional approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU1kc6KUeEOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# applying a simplified convolutional approach\n",
        "activate_function = \"relu\"\n",
        "model = create_simplified_cnn_network(embedded_sequences, activate_function)\n",
        "print(\"Model fitting - simplified convolutional neural network\")\n",
        "model.summary()\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
        "          epochs=10, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-1Hqy8jeT2s",
        "colab_type": "text"
      },
      "source": [
        "Train data using Deeper Convolutional neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elQGgPUdrqpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# applying a more complex convolutional approach\n",
        "convs = []\n",
        "filter_sizes = [3,4,5]\n",
        "\n",
        "for fsz in filter_sizes:\n",
        "    l_conv = Conv1D(filters=128,kernel_size=fsz,activation='relu')(embedded_sequences)\n",
        "    l_pool = MaxPooling1D(5)(l_conv)\n",
        "    convs.append(l_pool)\n",
        "\n",
        "activate_function = \"relu\"\n",
        "model = create_deeper_cnn_network(convs, activate_function)\n",
        "print(\"Model fitting - more complex convolutional neural network\")\n",
        "model.summary()\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
        "          epochs=20, batch_size=50)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}