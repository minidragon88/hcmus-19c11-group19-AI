{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdc9D156gB1A",
        "colab_type": "text"
      },
      "source": [
        "Prepare dependencies and global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGI_LAHQC4SS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "import re\n",
        "import sys\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from collections import defaultdict\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, concatenate, LSTM, GRU, Bidirectional\n",
        "from keras.models import Model\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "from keras import initializers\n",
        "\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "TSV_DIR = \"/content/drive/My Drive/Text Classification/Testground/data/imdb/labeledTrainData.tsv\"\n",
        "GLOVE_DIR = \"/content/drive/My Drive/Text Classification/Testground/data/glove/\"\n",
        "TRAINED_MODEL_DIR = \"/content/drive/My Drive/Text Classification/Testground/model/RNN/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s3I9mLfgPhC",
        "colab_type": "text"
      },
      "source": [
        "Prepare dependent functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgofIXFEbQCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_data(dir):\n",
        "  data_train = pd.read_csv(dir, sep='\\t')\n",
        "  return data_train\n",
        "\n",
        "def get_pretrained_glove_vector(dir):\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(dir, 'glove.6B.100d.txt'))\n",
        "\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))\n",
        "  return embeddings_index\n",
        "\n",
        "# remove some unwanted characters\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for dataset\n",
        "    Every dataset is lower cased except\n",
        "    \"\"\"\n",
        "    string = re.sub(b'\\\\\\\\', b'', string)   \n",
        "    string = re.sub(b\"\\'\", b\"\", string)    \n",
        "    string = re.sub(b\"\\\"\", b\"\", string)    \n",
        "    return string.strip().lower()\n",
        "\n",
        "def standardized_text(data_train):\n",
        "  # parser train data using BeautifulSub and remove html tags\n",
        "  text = BeautifulSoup(data_train.review[idx])\n",
        "  unclean_str = text.get_text().encode('ascii','ignore')\n",
        "  cleaned_str = clean_str(unclean_str)\n",
        "  # decode cleaned string to bytes-like type\n",
        "  return cleaned_str.decode('utf-8')\n",
        "\n",
        "def create_simplified_cnn_network(filter, activate_function=\"relu\"):\n",
        "  l_cov1= Conv1D(128, 5, activation=activate_function)(filter)\n",
        "  l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "  l_cov2 = Conv1D(128, 5, activation=activate_function)(l_pool1)\n",
        "  l_pool2 = MaxPooling1D(5)(l_cov2)\n",
        "  l_cov3 = Conv1D(128, 5, activation=activate_function)(l_pool2)\n",
        "  l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
        "  l_flat = Flatten()(l_pool3)\n",
        "  l_dense = Dense(128, activation=activate_function)(l_flat)\n",
        "  preds = Dense(2, activation='softmax')(l_dense)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['acc'])\n",
        "  return model\n",
        "\n",
        "def create_deeper_cnn_network(filter, activate_function=\"relu\"):\n",
        "  l_concatenate = concatenate(filter, axis=1)\n",
        "  l_cov1= Conv1D(128, 5, activation=activate_function)(l_concatenate)\n",
        "  l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "  l_cov2 = Conv1D(128, 5, activation=activate_function)(l_pool1)\n",
        "  l_pool2 = MaxPooling1D(30)(l_cov2)\n",
        "  l_flat = Flatten()(l_pool2)\n",
        "  l_dense = Dense(128, activation=activate_function)(l_flat)\n",
        "  preds = Dense(2, activation='softmax')(l_dense)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['acc'])\n",
        "  return model\n",
        "\n",
        "def save_model(model, save_dir, name):\n",
        "  if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "  model.save(save_dir + \"/\" + name)\n",
        "\n",
        "def load_model(load_dir, model_name):\n",
        "  return keras.models.load_model(load_dir + \"/\" + model_name)\n",
        "\n",
        "# our embedding layer will be a matrix with a row for each word\n",
        "# and a column for each element of the embedding.\n",
        "# Therefore, we need to specify how many dimensions one embedding has.\n",
        "# The version of Glove we loaded earlier has 100-dimensional vector.\n",
        "def get_embedding_layer():\n",
        "  embeddings_index = get_pretrained_glove_vector(GLOVE_DIR)\n",
        "  embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector        \n",
        "\n",
        "  # load pre-trained word embeddings into an Embedding layer\n",
        "  # By setting trainable = True, the network would learn the embedding by itself.     \n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                              EMBEDDING_DIM,\n",
        "                              weights=[embedding_matrix],\n",
        "                              input_length=MAX_SEQUENCE_LENGTH,\n",
        "                              trainable=True)\n",
        "  return embedding_layer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg7S5RTRgWjU",
        "colab_type": "text"
      },
      "source": [
        "Prepare data to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYnxG_OZDGPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = get_train_data(TSV_DIR)\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for idx in range(data_train.review.shape[0]):\n",
        "  texts.append(standardized_text(data_train))\n",
        "  labels.append(data_train.sentiment[idx])\n",
        "    \n",
        "\n",
        "# vectorize the text samples into a 2D integer tensor\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# split the data into a training set and a validation set\n",
        "labels = to_categorical(np.asarray(labels))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "# The training data & label for each data set\n",
        "# training_data  = x_train\n",
        "# training_label = y_train\n",
        "training_data = data[:-nb_validation_samples]\n",
        "training_label = labels[:-nb_validation_samples]\n",
        "\n",
        "# The validation data & label\n",
        "# validation_data     = x_val\n",
        "# validation_label    = y_val\n",
        "validation_data = data[-nb_validation_samples:]\n",
        "validation_label = labels[-nb_validation_samples:]\n",
        "\n",
        "print('Training and validation set number of positive and negative reviews')\n",
        "print (training_label.sum(axis=0))\n",
        "print (validation_label.sum(axis=0))\n",
        "\n",
        "# prepare embedding layer\n",
        "embedding_layer = get_embedding_layer()\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "print(\"...Data is ready to train\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrhP3dy3eMga",
        "colab_type": "text"
      },
      "source": [
        "# **Train data using LSTM model**\n",
        "The implemantation uses Bidirectional LSTM and concatenates both last output of LSTM outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU1kc6KUeEOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_layer = Bidirectional(LSTM(100))(embedded_sequences)\n",
        "preds = Dense(2, activation='softmax')(lstm_layer)\n",
        "model = Model(sequence_input, preds)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(\"model fitting - Bidirectional LSTM\")\n",
        "model.summary()\n",
        "model.fit(training_data, training_label, validation_data=(validation_data, validation_label),\n",
        "          epochs=10, batch_size=50)\n",
        "\n",
        "save_model(model, TRAINED_MODEL_DIR, \"LSTM_model.h5\")\n",
        "print(\"...Trained data model is saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-1Hqy8jeT2s",
        "colab_type": "text"
      },
      "source": [
        "# **Train data using Attention Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-vvruhYwfHa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "This model can only strictly run on Theano backend since tensorflow matrix dot product doesnâ€™t behave the same as np.dot.\n",
        "\n",
        "Make sure to change your backend to Theano first\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gac2xzkOpN_U",
        "colab_type": "text"
      },
      "source": [
        "Run this line of code first to check your Keras's Backend\n",
        "\n",
        "If your backend is Tensorflow, please Choose \"Factory reset Runtime\" in \"Runtime\" control tab\n",
        "\n",
        "If your backend is Theano, keep going!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDDvj0oUlQii",
        "colab_type": "code",
        "outputId": "218221e0-8d9a-44b0-82de-6c55862a7141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.environ['KERAS_BACKEND']='theano'\n",
        "\n",
        "from keras import __version__\n",
        "from keras import backend as K\n",
        "\n",
        "print('Using Keras version:', __version__, 'backend:', K.backend())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Keras version: 2.3.1 backend: tensorflow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAOhqZd3PAQt",
        "colab_type": "text"
      },
      "source": [
        "Build a custom Keras layer which would be used in Attention Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elQGgPUdrqpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# Attention GRU network\t\t  \n",
        "\n",
        "class AttLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.init = initializers.get('normal')\n",
        "        #self.input_spec = [InputSpec(ndim=3)]\n",
        "        super(AttLayer, self).__init__(** kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "      assert len(input_shape)==3\n",
        "      self.W = self.add_weight(name='kernel', \n",
        "                                    shape=(input_shape[-1],),\n",
        "                                    initializer='normal',\n",
        "                                    trainable=True)\n",
        "      super(AttLayer, self).build(input_shape)  \n",
        "\n",
        "    # This build function doesn't work\n",
        "    # def build(self, input_shape):\n",
        "    #     assert len(input_shape)==3\n",
        "    #     #self.W = self.init((input_shape[-1],1))\n",
        "    #     self.W = self.init((input_shape[-1],))\n",
        "    #     #self.input_spec = [InputSpec(shape=input_shape)]\n",
        "    #     self.trainable_weights = [self.W]\n",
        "    #     super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
        "\n",
        "    # This call function doesn't work\n",
        "    # Since The attention layer supports only Theano at the moment, not Tensorflow\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        eij = K.tanh(K.dot(x, self.W))\n",
        "\n",
        "        ai = K.exp(eij)\n",
        "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
        "\n",
        "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
        "        return weighted_input.sum(axis=1)\n",
        "\n",
        "    # # Replace call function\n",
        "    # def call(self, x, mask=None):\n",
        "    #   eij = K.tanh(K.squeeze(K.dot(x, K.expand_dims(self.W)), axis=-1))\n",
        "      \n",
        "    #   ai = K.exp(eij)\n",
        "    #   weights = ai/K.expand_dims(K.sum(ai, axis=1),1)\n",
        "      \n",
        "    #   weighted_input = x*K.expand_dims(weights,2)\n",
        "    #   return K.sum(weighted_input, axis=1)\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUXUECeroylO",
        "colab_type": "text"
      },
      "source": [
        "Train model using Attention GRU model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAUBrpffP86X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gru_layer = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
        "# add an attention layer on top of GRU Output\n",
        "att_layer = AttLayer()(gru_layer)\n",
        "\n",
        "preds = Dense(2, activation='softmax')(att_layer)\n",
        "model = Model(sequence_input, preds)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])\n",
        "\n",
        "print(\"model fitting - attention GRU network\")\n",
        "model.summary()\n",
        "model.fit(training_data, training_label, validation_data=(validation_data, validation_label),\n",
        "          epochs=10, batch_size=50)\n",
        "\n",
        "save_model(model, TRAINED_MODEL_DIR, \"GRU_model.h5\")\n",
        "print(\"...Trained data model is saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}