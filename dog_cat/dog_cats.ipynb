{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dog_cats.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc5b3uuoxpiY",
        "colab_type": "text"
      },
      "source": [
        "Prepare dependencies and global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbUQJTj20Pt5",
        "colab_type": "code",
        "outputId": "1279396a-a208-418a-bd96-d9e8b0f064d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "import os\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation, Conv2D, MaxPooling2D,BatchNormalization\n",
        "import pickle\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam, SGD\n",
        "from tensorflow import keras\n",
        "\n",
        "raw_train_dir = \"/content/drive/My Drive/dog_cat/raw_train\"\n",
        "train_dir = \"/content/drive/My Drive/dog_cat/train\"\n",
        "raw_test_dir = \"/content/drive/My Drive/dog_cat/raw_test\"\n",
        "test_dir = \"/content/drive/My Drive/dog_cat/test\"\n",
        "train_data_file = \"/content/drive/My Drive/dog_cat/train_data\"\n",
        "train_label_file = \"/content/drive/My Drive/dog_cat/train_label\"\n",
        "test_data_file = \"/content/drive/My Drive/dog_cat/test_data\"\n",
        "trained_model_dir = \"/content/drive/My Drive/dog_cat/trained_model\"\n",
        "\n",
        "def gray_out_and_save_image(original_dir, destionation_dir):\n",
        "  if not os.path.exists(destionation_dir):\n",
        "    os.makedirs(destionation_dir)\n",
        "  for file_name in os.listdir(original_dir):\n",
        "      original_arr = cv2.imread(os.path.join(original_dir, file_name), cv2.IMREAD_GRAYSCALE)\n",
        "      new_arr = cv2.resize(original_arr, dsize=(80, 80),interpolation = cv2.IMREAD_GRAYSCALE)\n",
        "      plt.imsave(destionation_dir + \"/\" + file_name, new_arr, cmap=\"gray\")\n",
        "\n",
        "def get_train_data_and_label(train_dir):\n",
        "  data = []\n",
        "  label = []\n",
        "  get_label = lambda prefix : int(prefix == 'dog')\n",
        "  for file_name in os.listdir(train_dir):\n",
        "        prefix = file_name.split(\".\")[0]\n",
        "        labeled = get_label(prefix)\n",
        "        img_array = cv2.imread(os.path.join(train_dir, file_name), cv2.IMREAD_GRAYSCALE)\n",
        "        data.append(img_array)\n",
        "        label.append(labeled)\n",
        "  return data, label\n",
        "\n",
        "def serialize_data(data, file_path):\n",
        "  open(file_path, 'wb').close()\n",
        "  with open(file_path, \"wb\") as file:\n",
        "    pickle.dump(data, file)\n",
        "\n",
        "def deserialize_data(file_path):\n",
        "  with open(file_path, \"rb\") as data_file:\n",
        "    return pickle.load(data_file)\n",
        "\n",
        "def transform_label_to_array(labels):\n",
        "  temp_result = []\n",
        "  for label in labels:\n",
        "    if label == 0:\n",
        "      temp_result.append([0,1])\n",
        "    else:\n",
        "      temp_result.append([1,0])\n",
        "  return np.array(temp_result)\n",
        "\n",
        "def create_callbacks():\n",
        "  earlystop = EarlyStopping(patience=10)\n",
        "  learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
        "                                            patience=2, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)\n",
        "  return [earlystop, learning_rate_reduction]\n",
        "\n",
        "def create_cnn_network(input_shape, activate_function=\"relu\"):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(64, (3, 3), activation=activate_function, input_shape=input_shape))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  \n",
        "  model.add(Conv2D(64, (3, 3), activation=activate_function))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  \n",
        "  model.add(Conv2D(128, (3, 3), activation=activate_function))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  \n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512, activation=activate_function))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "  opt = keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "  model.compile(optimizer=opt,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def save_model(model, save_dir, name):\n",
        "  if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "  model.save(save_dir + \"/\" + name)\n",
        "\n",
        "def load_model(trained_model_dir, activation_function):\n",
        "  return keras.models.load_model(trained_model_dir + \"/\" + activation_function + \"_model.h5\")\n",
        "\n",
        "callbacks = create_callbacks()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR5Kym6HAq24",
        "colab_type": "text"
      },
      "source": [
        "Transform to grey and store in trained set\n",
        "\n",
        "This is one time action, so uncommented to run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNIj_yV7BQ4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gray_out_and_save_image(raw_train_dir, train_dir)\n",
        "# gray_out_and_save_image(raw_test_dir, test_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nzr1yC8BRpi",
        "colab_type": "text"
      },
      "source": [
        "Dump data using pickle to avoid loading so many file for next run\n",
        "\n",
        "Dump both train, test and label data\n",
        "\n",
        "This is one time action"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAEX7dAhBR_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_data, label_data = get_train_data_and_label(train_dir)\n",
        "# test_data, temp = get_train_data_and_label(test_dir)\n",
        "# serialize_data(train_data, train_data_file)\n",
        "# serialize_data(label_data, train_label_file)\n",
        "# serialize_data(test_data, test_data_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4O6uAOCIKLE",
        "colab_type": "text"
      },
      "source": [
        "Prepare data to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06yEcIJEIQM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data\n",
        "train_data = deserialize_data(train_data_file)\n",
        "label_data = deserialize_data(train_label_file)\n",
        "test_data = deserialize_data(test_data_file)\n",
        "\n",
        "# Transform into array and resharp to 3 dimension array with size 80 x 80 x 1\n",
        "train_data = np.array(train_data).reshape(-1, 80, 80, 1)\n",
        "test_data = np.array(test_data).reshape(-1, 80, 80, 1)\n",
        "label_data = np.array(label_data)\n",
        "\n",
        "# Shuffle train data and label in order to perform train model - 90% train - 10% test\n",
        "from sklearn.model_selection import train_test_split\n",
        "data_to_train, data_to_verify, label_to_train, label_to_verify = train_test_split(train_data, label_data, test_size=0.1)\n",
        "\n",
        "# Divide to matrix with value from 0..1\n",
        "data_to_train = data_to_train / 255.0\n",
        "data_to_verify = data_to_verify / 255.0\n",
        "\n",
        "# Train form label to 1x2 array\n",
        "label_to_train = transform_label_to_array(label_to_train)\n",
        "label_to_verify = transform_label_to_array(label_to_verify)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erzbRS4aMZh7",
        "colab_type": "text"
      },
      "source": [
        "Train and save model with relu activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhugCHnTMhks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation_function = \"relu\"\n",
        "model = create_cnn_network(data_to_train.shape[1:], activation_function)\n",
        "model.fit(data_to_train, label_to_train, epochs=100, batch_size=64, validation_data=((data_to_verify, label_to_verify)), callbacks = callbacks)\n",
        "save_model(model, trained_model_dir, activation_function + \"_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7RzYOd9Avys",
        "colab_type": "text"
      },
      "source": [
        "Train and save model with sigmoid activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "573gfLRpAwKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation_function = \"sigmoid\"\n",
        "model = create_cnn_network(data_to_train.shape[1:], activation_function)\n",
        "model.fit(data_to_train, label_to_train, epochs=100, batch_size=64, validation_data=((data_to_verify, label_to_verify)), callbacks = callbacks)\n",
        "save_model(model, trained_model_dir, activation_function + \"_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19X5MK0pKl2P",
        "colab_type": "text"
      },
      "source": [
        "Train and save model with softplus activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g04UoDnkKkVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation_function = \"softplus\"\n",
        "model = create_cnn_network(data_to_train.shape[1:], activation_function)\n",
        "model.fit(data_to_train, label_to_train, epochs=100, batch_size=64, validation_data=((data_to_verify, label_to_verify)), callbacks = callbacks)\n",
        "save_model(model, trained_model_dir, activation_function + \"_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ3CCEF-ArhK",
        "colab_type": "text"
      },
      "source": [
        "Get statistic of trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgHaIzMH8F_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = np.copy(train_data)\n",
        "all_data = all_data / 255.0\n",
        "all_labels = transform_label_to_array(label_data)\n",
        "\n",
        "activation_function = \"relu\"\n",
        "print(activation_function.upper())\n",
        "model = load_model(trained_model_dir, activation_function)\n",
        "print(model.summary())\n",
        "scores = model.evaluate(data_to_verify, label_to_verify)\n",
        "print(\"Result on verify data\")\n",
        "print(\"Loss : \" + str(scores[0]) + \" Accuracy: \" + str(scores[1]))\n",
        "scores = model.evaluate(all_data, all_labels)\n",
        "print(\"Result on all data\")\n",
        "print(\"Loss : \" + str(scores[0]) + \" Accuracy: \" + str(scores[1]))\n",
        "\n",
        "activation_function = \"sigmoid\"\n",
        "print(activation_function.upper())\n",
        "model = load_model(trained_model_dir, activation_function)\n",
        "print(model.summary())\n",
        "scores = model.evaluate(data_to_verify, label_to_verify)\n",
        "print(\"Result on verify data\")\n",
        "print(\"Loss : \" + str(scores[0]) + \" Accuracy: \" + str(scores[1]))\n",
        "scores = model.evaluate(all_data, all_labels)\n",
        "print(\"Result on all data\")\n",
        "print(\"Loss : \" + str(scores[0]) + \" Accuracy: \" + str(scores[1]))\n",
        "\n",
        "activation_function = \"softplus\"\n",
        "print(activation_function.upper())\n",
        "model = load_model(trained_model_dir, activation_function)\n",
        "print(model.summary())\n",
        "scores = model.evaluate(data_to_verify, label_to_verify)\n",
        "print(\"Result on verify data\")\n",
        "print(\"Loss : \" + str(scores[0]) + \" Accuracy: \" + str(scores[1]))\n",
        "scores = model.evaluate(all_data, all_labels)\n",
        "print(\"Result on all data\")\n",
        "print(\"Loss : \" + str(scores[0]) + \" Accuracy: \" + str(scores[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}